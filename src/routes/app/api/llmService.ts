import Groq from 'groq-sdk';
import OpenAI from 'openai';
import { error } from '@sveltejs/kit';


// Initialize Groq and OpenAI clients
const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY,
});

const openai = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: process.env.OPENROUTER_API_KEY,
  defaultHeaders: {},
});


/**
 * Retries a function with exponential backoff.
 * @param fn The asynchronous function to retry.
 * @param retries Number of retry attempts.
 * @param delay Initial delay in milliseconds.
 * @param factor Exponential factor.
 * @param jitter Randomness added to delay.
 */
async function retryWithBackoff<T>(
  fn: () => Promise<T>,
  retries: number = 5,
  delay: number = 1000,
  factor: number = 2,
  jitter: number = 300
): Promise<T> {
  try {
    return await fn();
  } catch (error:any) {
    console.warn(`Caught error: ${error.message}`);
    if (retries <= 0) {
      // throw error;
      throw error(420, 'Upload was not successful, please try again');
    }

    // Optionally, inspect the error to decide if it's retryable
    // For example, retry only on network errors or specific status codes
    // if (!isRetryableError(error)) {
    //   throw error;
    // }

    // Calculate delay with exponential backoff and jitter
    const backoffDelay = delay * Math.pow(factor, retries - 1) + Math.random() * jitter;
    console.warn(`Retrying in ${backoffDelay.toFixed(0)} ms... (${retries} retries left)`);
    await new Promise((resolve) => setTimeout(resolve, backoffDelay));
    return retryWithBackoff(fn, retries - 1, delay, factor, jitter);
  }
}


/**
 * Calls the LLM using either Groq or OpenAI based on the useGroq flag.
 * Implements retry logic with exponential backoff for robustness.
 *
 * @param system_prompt The system prompt for the LLM.
 * @param user_prompt The user prompt for the LLM.
 * @param useGroq Flag to determine which API to use.
 * @returns The content generated by the LLM.
 */
export async function callLLM(
  system_prompt: string,
  user_prompt: string,
  useGroq: boolean
): Promise<string> {
  // Define the API call as a function to pass to the retry utility
  const apiCall = async () => {
    let chatCompletion;
    if (useGroq) {
      chatCompletion = await groq.chat.completions.create({
        model: 'llama-3.1-70b-versatile', // or 'llama3-70b-8192'
        messages: [
          {
            role: 'system',
            content: system_prompt,
          },
          {
            role: 'user',
            content: user_prompt,
          },
        ],
        temperature: 0,
        // max_tokens: 10000, // Uncomment if needed
        top_p: 1,
        frequency_penalty: 0,
        presence_penalty: 0,
      });
    } else {
      chatCompletion = await openai.chat.completions.create({
        model: 'google/gemini-flash-1.5',
        messages: [
          {
            role: 'system',
            content: system_prompt,
          },
          {
            role: 'user',
            content: user_prompt,
          },
        ],
        temperature: 0,
        // max_tokens: 10000, // Uncomment if needed
        top_p: 1,
        frequency_penalty: 0,
        presence_penalty: 0,
      });
    }

    const content = chatCompletion.choices[0].message.content;
    if (!content) {
      throw new Error('No content in chatCompletion');
    }

    return content;
  };

  // Wrap the API call with retry logic
  try {
    const result = await retryWithBackoff(apiCall, 5, 1000, 2, 300);
    return result;
  } catch (error) {
    console.error('Failed to get LLM response after retries:', error);
    throw error; // Re-throw the error after retries are exhausted
  }
}
